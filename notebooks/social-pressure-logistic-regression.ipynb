{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Pressure, Voter Turnout, and Logistic Regression\n",
    "\n",
    "**Persuasion at Scale** | Week 4, Lecture 8 | 2026-02-16\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chrishwiggins/social-pressure-voting/blob/main/notebooks/social-pressure-logistic-regression.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks through the data from two assigned readings:\n",
    "\n",
    "1. **Gerber, Green, and Larimer (2008)** \"Social Pressure and Voter Turnout\" (APSR)\n",
    "2. **Coppock, Hill, and Vavreck (2020/2024)** on campaign persuasion effects\n",
    "\n",
    "We will:\n",
    "- Load the **actual experimental data** from GGL (2008): 344,084 individuals, five treatment groups\n",
    "- Reproduce the paper's main results\n",
    "- Introduce **logistic regression** as the right tool for binary outcomes\n",
    "- Build intuition visually before touching any math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 13\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Experiment\n",
    "\n",
    "In August 2006, Gerber, Green, and Larimer ran a field experiment with **344,084 registered voters** in Michigan. Households were randomly assigned to one of five groups:\n",
    "\n",
    "| Group | What they received | N |\n",
    "|-------|-------------------|---|\n",
    "| **Control** | Nothing | 191,243 |\n",
    "| **Civic Duty** | \"DO YOUR CIVIC DUTY - VOTE!\" | 38,218 |\n",
    "| **Hawthorne** | \"YOU ARE BEING STUDIED\" (told researchers would check if they voted) | 38,204 |\n",
    "| **Self** | Showed the recipient's own past voting record | 38,218 |\n",
    "| **Neighbors** | Showed the recipient AND their neighbors' past voting records | 38,201 |\n",
    "\n",
    "The key outcome: **did they vote in the August 2006 primary?** (binary: yes/no)\n",
    "\n",
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the actual replication data from ISPS Yale\n",
    "url = 'http://hdl.handle.net/10079/d3669799-4537-411e-b175-d9e837324c35'\n",
    "df = pd.read_csv(url)\n",
    "print(f'Loaded {len(df):,} individual records')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "df['treatment'] = df['treatment'].str.strip()\n",
    "df['voted_binary'] = (df['voted'] == 'Yes').astype(int)\n",
    "df['age'] = 2006 - df['yob']  # approximate age at time of election\n",
    "\n",
    "# Past voting: convert yes/no to 1/0 for covariates\n",
    "for col in ['g2000', 'g2002', 'g2004', 'p2000', 'p2002', 'p2004']:\n",
    "    df[col + '_bin'] = (df[col].str.strip().str.lower() == 'yes').astype(int)\n",
    "\n",
    "print(f'Age range: {df[\"age\"].min()} to {df[\"age\"].max()}')\n",
    "print(f'Overall turnout: {df[\"voted_binary\"].mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "\n",
    "Each row is one registered voter. The key columns:\n",
    "- `treatment`: which experimental group they were assigned to\n",
    "- `voted`: did they vote in the August 2006 primary? (our **outcome**)\n",
    "- `g2000`, `g2002`, `g2004`: voted in general elections in 2000, 2002, 2004?\n",
    "- `p2000`, `p2002`, `p2004`: voted in primary elections in 2000, 2002, 2004?\n",
    "- `yob`: year of birth\n",
    "- `hh_id`, `hh_size`: household ID and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment group sizes\n",
    "group_order = ['Control', 'Civic Duty', 'Hawthorne', 'Self', 'Neighbors']\n",
    "colors = ['#95a5a6', '#3498db', '#2ecc71', '#e67e22', '#e74c3c']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "counts = [len(df[df['treatment'] == t]) for t in group_order]\n",
    "bars = ax.bar(group_order, counts, color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2000,\n",
    "            f'{count:,}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Number of Individuals')\n",
    "ax.set_title('Sample Sizes by Treatment Group')\n",
    "ax.set_ylim(0, 220000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the control group is about 5x larger than each treatment group. This is a deliberate design choice: it's cheap to not send mail, so the researchers allocated more people to control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Main Result (Difference in Means)\n",
    "\n",
    "The simplest analysis of an RCT: compare the average outcome across groups.\n",
    "\n",
    "Because treatment was **randomly assigned**, any difference in turnout between groups is a valid estimate of the **causal effect** of the mailer.\n",
    "\n",
    "This is the **intent-to-treat (ITT) effect**: the effect of being *assigned* the treatment (not necessarily reading the mailer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce Table 2 from the paper\n",
    "control_rate = df[df['treatment'] == 'Control']['voted_binary'].mean()\n",
    "\n",
    "print('=' * 60)\n",
    "print('Replicating GGL (2008), Table 2')\n",
    "print('=' * 60)\n",
    "print(f'{\"Group\":15s} {\"Turnout\":>10s} {\"ITT Effect\":>12s} {\"N\":>10s}')\n",
    "print('-' * 60)\n",
    "\n",
    "for t in group_order:\n",
    "    sub = df[df['treatment'] == t]\n",
    "    rate = sub['voted_binary'].mean()\n",
    "    effect = (rate - control_rate) * 100 if t != 'Control' else 0\n",
    "    effect_str = f'{effect:+.1f} pp' if t != 'Control' else '(baseline)'\n",
    "    print(f'{t:15s} {rate*100:9.1f}% {effect_str:>12s} {len(sub):>10,}')\n",
    "\n",
    "print('-' * 60)\n",
    "print(f'\\nNeighbors effect: +8.1 percentage points over control')\n",
    "print(f'That is a {8.1/29.7*100:.0f}% relative increase in turnout!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: turnout by treatment group\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rates = [df[df['treatment'] == t]['voted_binary'].mean() * 100 for t in group_order]\n",
    "bars = ax.bar(group_order, rates, color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "# Add rate labels\n",
    "for bar, rate in zip(bars, rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add horizontal line at control rate\n",
    "ax.axhline(y=rates[0], color='gray', linestyle='--', alpha=0.7, label=f'Control baseline ({rates[0]:.1f}%)')\n",
    "\n",
    "ax.set_ylabel('Turnout Rate (%)')\n",
    "ax.set_title('Voter Turnout by Treatment Group\\nGerber, Green, and Larimer (2008)', fontsize=14)\n",
    "ax.set_ylim(25, 42)\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Neighbors\" treatment, which showed recipients their neighbors' past voting records and promised to send an updated version after the election, produced the largest effect: **+8.1 percentage points** over the control group.\n",
    "\n",
    "The treatments are ordered by social pressure intensity:\n",
    "- Civic Duty (weakest): generic civic appeal\n",
    "- Hawthorne: you're being watched\n",
    "- Self: we know YOUR voting history\n",
    "- Neighbors (strongest): we know your neighbors' history too, and we'll tell them yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking randomization\n",
    "\n",
    "One advantage of the real data: we can verify that randomization \"worked.\" If treatment was truly random, pre-treatment covariates should be balanced across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance check: past voting history by treatment group\n",
    "balance_vars = ['g2004_bin', 'g2002_bin', 'g2000_bin', 'p2004_bin', 'age']\n",
    "balance_labels = ['Voted 2004 Gen', 'Voted 2002 Gen', 'Voted 2000 Gen',\n",
    "                  'Voted 2004 Primary', 'Age']\n",
    "\n",
    "print(f'{\"Variable\":20s}', end='')\n",
    "for t in group_order:\n",
    "    print(f'{t:>12s}', end='')\n",
    "print()\n",
    "print('-' * 80)\n",
    "\n",
    "for var, label in zip(balance_vars, balance_labels):\n",
    "    print(f'{label:20s}', end='')\n",
    "    for t in group_order:\n",
    "        val = df[df['treatment'] == t][var].mean()\n",
    "        if var == 'age':\n",
    "            print(f'{val:12.1f}', end='')\n",
    "        else:\n",
    "            print(f'{val*100:11.1f}%', end='')\n",
    "    print()\n",
    "\n",
    "print('\\nVery similar across groups = randomization worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual balance check: age distribution by group\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4), sharey=True)\n",
    "\n",
    "for ax, t, c in zip(axes, group_order, colors):\n",
    "    sub = df[df['treatment'] == t]\n",
    "    ax.hist(sub['age'], bins=range(18, 100, 2), color=c, alpha=0.8, density=True)\n",
    "    ax.set_title(t, fontsize=11)\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.axvline(sub['age'].mean(), color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "axes[0].set_ylabel('Density')\n",
    "fig.suptitle('Age Distributions Look Identical Across Groups (Randomization Worked)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Why Not Just Stop Here?\n",
    "\n",
    "The difference-in-means analysis is perfectly valid for an RCT. So why bother with regression at all?\n",
    "\n",
    "Three reasons:\n",
    "1. **Precision**: Adding pre-treatment covariates can reduce noise and give tighter confidence intervals\n",
    "2. **Subgroup effects**: We might want to know if the treatment works differently for different people\n",
    "3. **The outcome is binary**: Turnout is either 0 or 1. Regular linear regression can predict values outside [0, 1]. Logistic regression respects the binary nature of the outcome.\n",
    "\n",
    "Let's see why #3 matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with the linear probability model\n",
    "\n",
    "A **linear probability model (LPM)** just runs ordinary regression on a binary outcome:\n",
    "\n",
    "$$\\text{voted}_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\varepsilon_i$$\n",
    "\n",
    "This gives us an easy-to-interpret coefficient: $\\beta_1$ is the change in probability of voting.\n",
    "\n",
    "But it has a conceptual problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example to illustrate the problem\n",
    "# Turnout vs age: older people vote more\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Take a random sample for plotting (full dataset is too dense)\n",
    "sample = df.sample(3000, random_state=42)\n",
    "\n",
    "# Left: scatter of actual data with linear fit\n",
    "ax = axes[0]\n",
    "jitter = np.random.uniform(-0.05, 0.05, len(sample))\n",
    "ax.scatter(sample['age'], sample['voted_binary'] + jitter,\n",
    "           alpha=0.1, s=10, color='steelblue')\n",
    "\n",
    "# Linear fit\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "age_range = np.linspace(18, 95, 200)\n",
    "b, m = polyfit(sample['age'], sample['voted_binary'], 1)\n",
    "ax.plot(age_range, b + m * age_range, 'r-', linewidth=2.5, label='Linear fit')\n",
    "\n",
    "ax.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(1, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.fill_between(age_range, -0.3, 0, alpha=0.1, color='red')\n",
    "ax.fill_between(age_range, 1, 1.3, alpha=0.1, color='red')\n",
    "ax.text(80, -0.15, 'Impossible!\\n(probability < 0)', color='red', fontsize=10, ha='center')\n",
    "ax.text(80, 1.1, 'Impossible!\\n(probability > 1)', color='red', fontsize=10, ha='center')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Voted (0/1)')\n",
    "ax.set_title('Linear Probability Model\\n(can predict outside [0, 1])')\n",
    "ax.set_ylim(-0.3, 1.3)\n",
    "ax.legend()\n",
    "\n",
    "# Right: the logistic (sigmoid) function\n",
    "ax = axes[1]\n",
    "z = np.linspace(-6, 6, 200)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "ax.plot(z, sigmoid, 'b-', linewidth=3)\n",
    "ax.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(1, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.fill_between(z, 0, 1, alpha=0.05, color='green')\n",
    "ax.text(0, 0.5, '  always between 0 and 1', color='green', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Linear predictor (z)')\n",
    "ax.set_ylabel('Predicted probability')\n",
    "ax.set_title('Logistic (Sigmoid) Function\\n$\\sigma(z) = 1 / (1 + e^{-z})$')\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot shows the problem: a straight line through binary (0/1) data will eventually predict probabilities below 0 or above 1.\n",
    "\n",
    "The right plot shows the solution: the **logistic function** (also called the sigmoid) squashes any input into the range (0, 1). It's an S-shaped curve that:\n",
    "- Approaches 0 as the input goes to $-\\infty$\n",
    "- Approaches 1 as the input goes to $+\\infty$\n",
    "- Equals exactly 0.5 when the input is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: What Logistic Regression Actually Does\n",
    "\n",
    "Instead of modeling probability directly:\n",
    "\n",
    "$$P(\\text{voted}=1) = \\beta_0 + \\beta_1 x \\quad \\text{(linear, can go outside [0,1])}$$\n",
    "\n",
    "Logistic regression models the **log-odds**:\n",
    "\n",
    "$$\\log\\left(\\frac{P(\\text{voted}=1)}{1 - P(\\text{voted}=1)}\\right) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "Equivalently, the predicted probability is:\n",
    "\n",
    "$$P(\\text{voted}=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
    "\n",
    "Let's build intuition for what \"odds\" and \"log-odds\" mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: probability -> odds -> log-odds\n",
    "probs = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "odds = probs / (1 - probs)\n",
    "log_odds = np.log(odds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "# Probability scale\n",
    "ax = axes[0]\n",
    "ax.barh(range(len(probs)), probs, color='steelblue', height=0.6)\n",
    "ax.set_yticks(range(len(probs)))\n",
    "ax.set_yticklabels([f'{p:.0%}' for p in probs])\n",
    "ax.set_xlabel('Probability')\n",
    "ax.set_title('Probability\\n(bounded 0 to 1)')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Odds scale\n",
    "ax = axes[1]\n",
    "ax.barh(range(len(probs)), odds, color='#e67e22', height=0.6)\n",
    "ax.set_yticks(range(len(probs)))\n",
    "ax.set_yticklabels([f'{o:.2f}' for o in odds])\n",
    "ax.set_xlabel('Odds')\n",
    "ax.set_title('Odds = p/(1-p)\\n(bounded 0 to $\\infty$)')\n",
    "\n",
    "# Log-odds scale\n",
    "ax = axes[2]\n",
    "bar_colors = ['#e74c3c' if lo < 0 else '#2ecc71' for lo in log_odds]\n",
    "ax.barh(range(len(probs)), log_odds, color=bar_colors, height=0.6)\n",
    "ax.set_yticks(range(len(probs)))\n",
    "ax.set_yticklabels([f'{lo:+.2f}' for lo in log_odds])\n",
    "ax.set_xlabel('Log-odds')\n",
    "ax.set_title('Log-odds = log(p/(1-p))\\n(unbounded: $-\\infty$ to $+\\infty$)')\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key insight: log-odds is the scale where logistic regression is linear.')\n",
    "print('Positive log-odds = more likely than not (p > 50%)')\n",
    "print('Negative log-odds = less likely than not (p < 50%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why log-odds?** It's the transformation that maps probabilities (bounded between 0 and 1) to an unbounded scale ($-\\infty$ to $+\\infty$). On the log-odds scale, it makes sense to fit a linear model.\n",
    "\n",
    "Think of it this way:\n",
    "- A probability of 50% = odds of 1:1 = log-odds of 0\n",
    "- A probability of 90% = odds of 9:1 = log-odds of +2.2\n",
    "- A probability of 10% = odds of 1:9 = log-odds of -2.2\n",
    "\n",
    "The symmetry is nice: 90% and 10% are equally far from 50%, just in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Logistic Regression on the GGL Data\n",
    "\n",
    "Now let's fit logistic regression to the real experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# First: the simple linear probability model (OLS) for comparison\n",
    "lpm = smf.ols('voted_binary ~ C(treatment, Treatment(reference=\"Control\"))', data=df).fit()\n",
    "\n",
    "print('=' * 65)\n",
    "print('LINEAR PROBABILITY MODEL (OLS)')\n",
    "print('Coefficients are changes in probability of voting')\n",
    "print('=' * 65)\n",
    "for name, coef in lpm.params.items():\n",
    "    if 'Intercept' in name:\n",
    "        print(f'  Control baseline:  {coef:.4f} ({coef*100:.1f}%)')\n",
    "    else:\n",
    "        # Extract treatment name\n",
    "        tname = name.split('T.\")[1].rstrip('\"').rstrip(']')\n",
    "        print(f'  {tname:15s}:  {coef:+.4f} ({coef*100:+.1f} pp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: logistic regression\n",
    "logit = smf.logit('voted_binary ~ C(treatment, Treatment(reference=\"Control\"))', data=df).fit(disp=0)\n",
    "\n",
    "print('=' * 65)\n",
    "print('LOGISTIC REGRESSION')\n",
    "print('Coefficients are changes in LOG-ODDS of voting')\n",
    "print('=' * 65)\n",
    "for name, coef in logit.params.items():\n",
    "    if 'Intercept' in name:\n",
    "        p = np.exp(coef) / (1 + np.exp(coef))\n",
    "        print(f'  Control (intercept): {coef:.4f} (log-odds)  ->  {p*100:.1f}% (probability)')\n",
    "    else:\n",
    "        tname = name.split('T.\")[1].rstrip('\"').rstrip(']')\n",
    "        or_val = np.exp(coef)\n",
    "        print(f'  {tname:15s}: {coef:+.4f} (log-odds)  odds ratio: {or_val:.3f}')\n",
    "\n",
    "print()\n",
    "print('An odds ratio > 1 means the treatment INCREASES the odds of voting.')\n",
    "print('For example, OR=1.45 means 45% higher odds of voting than control.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LPM and Logistic side by side\n",
    "treatment_names = ['Civic Duty', 'Hawthorne', 'Self', 'Neighbors']\n",
    "\n",
    "# Get marginal effects from logistic (average predicted probability differences)\n",
    "logit_margins = logit.get_margeff()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: LPM coefficients (= difference in means)\n",
    "ax = axes[0]\n",
    "lpm_effects = [lpm.params[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'] * 100\n",
    "               for t in treatment_names]\n",
    "lpm_ci = [lpm.conf_int().loc[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'] * 100\n",
    "          for t in treatment_names]\n",
    "lpm_errors = [(e[1] - e[0])/2 for e in lpm_ci]\n",
    "\n",
    "ax.barh(treatment_names, lpm_effects, xerr=lpm_errors,\n",
    "        color=colors[1:], edgecolor='white', linewidth=1.5, capsize=5)\n",
    "ax.set_xlabel('Effect on Turnout (percentage points)')\n",
    "ax.set_title('Linear Probability Model\\n(coefficient = pp change)')\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Right: Logistic regression (odds ratios)\n",
    "ax = axes[1]\n",
    "odds_ratios = [np.exp(logit.params[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'])\n",
    "               for t in treatment_names]\n",
    "logit_ci_vals = logit.conf_int()\n",
    "or_lower = [np.exp(logit_ci_vals.loc[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]', 0])\n",
    "            for t in treatment_names]\n",
    "or_upper = [np.exp(logit_ci_vals.loc[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]', 1])\n",
    "            for t in treatment_names]\n",
    "or_errors = [[o - l for o, l in zip(odds_ratios, or_lower)],\n",
    "             [u - o for o, u in zip(odds_ratios, or_upper)]]\n",
    "\n",
    "ax.barh(treatment_names, odds_ratios, xerr=or_errors,\n",
    "        color=colors[1:], edgecolor='white', linewidth=1.5, capsize=5)\n",
    "ax.set_xlabel('Odds Ratio (vs Control)')\n",
    "ax.set_title('Logistic Regression\\n(odds ratio: >1 means higher turnout)')\n",
    "ax.axvline(1, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Both models tell the same story: more social pressure -> more turnout.')\n",
    "print('The ranking is identical; only the scale of the coefficients differs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the numbers so similar?\n",
    "\n",
    "When the baseline probability is near 30% and the effects are modest (a few percentage points), the linear probability model and logistic regression give nearly identical answers.\n",
    "\n",
    "The logistic model matters more when:\n",
    "- Baseline rates are very high or very low (near 0% or 100%)\n",
    "- You have continuous covariates (like age) where the linear model might predict outside [0, 1]\n",
    "- You want predicted probabilities for new individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Adding Covariates\n",
    "\n",
    "In an RCT, adding pre-treatment covariates doesn't change the *bias* of our estimate (randomization already handles that), but it can improve **precision**.\n",
    "\n",
    "Past voting behavior is the strongest predictor of future voting. Let's add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with covariates\n",
    "logit_full = smf.logit(\n",
    "    'voted_binary ~ C(treatment, Treatment(reference=\"Control\"))'\n",
    "    ' + g2000_bin + g2002_bin + g2004_bin'\n",
    "    ' + p2000_bin + p2002_bin + p2004_bin'\n",
    "    ' + age + hh_size',\n",
    "    data=df\n",
    ").fit(disp=0)\n",
    "\n",
    "print('=' * 65)\n",
    "print('LOGISTIC REGRESSION WITH COVARIATES')\n",
    "print('=' * 65)\n",
    "print(logit_full.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confidence intervals: with and without covariates\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "y_positions = np.arange(len(treatment_names))\n",
    "offset = 0.15\n",
    "\n",
    "for i, t in enumerate(treatment_names):\n",
    "    key = f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'\n",
    "\n",
    "    # Without covariates\n",
    "    coef1 = logit.params[key]\n",
    "    ci1 = logit.conf_int().loc[key]\n",
    "    ax.plot([ci1[0], ci1[1]], [i + offset, i + offset], 'o-',\n",
    "            color='steelblue', linewidth=2.5, markersize=6)\n",
    "\n",
    "    # With covariates\n",
    "    coef2 = logit_full.params[key]\n",
    "    ci2 = logit_full.conf_int().loc[key]\n",
    "    ax.plot([ci2[0], ci2[1]], [i - offset, i - offset], 's-',\n",
    "            color='#e74c3c', linewidth=2.5, markersize=6)\n",
    "\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(treatment_names)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('Log-odds Coefficient (with 95% CI)')\n",
    "ax.set_title('Treatment Effects: With vs Without Covariates')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='steelblue', label='Treatment only')\n",
    "red_patch = mpatches.Patch(color='#e74c3c', label='+ voting history, age, hh_size')\n",
    "ax.legend(handles=[blue_patch, red_patch], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Red intervals are narrower: covariates absorb noise, giving more precise estimates.')\n",
    "print('The point estimates barely change (because randomization already removed bias).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Predicted Probabilities\n",
    "\n",
    "One of the nicest features of logistic regression: we can compute predicted probabilities for specific types of voters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted turnout by age and treatment group\n",
    "ages = np.arange(20, 90)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for t, c in zip(group_order, colors):\n",
    "    # Create prediction data: median voter characteristics, varying age\n",
    "    pred_df = pd.DataFrame({\n",
    "        'treatment': t,\n",
    "        'g2000_bin': 1,  # typical voter: voted in past generals\n",
    "        'g2002_bin': 1,\n",
    "        'g2004_bin': 1,\n",
    "        'p2000_bin': 0,  # but not in past primaries\n",
    "        'p2002_bin': 0,\n",
    "        'p2004_bin': 0,\n",
    "        'age': ages,\n",
    "        'hh_size': 2\n",
    "    })\n",
    "    pred_probs = logit_full.predict(pred_df)\n",
    "    lw = 3 if t in ['Control', 'Neighbors'] else 1.5\n",
    "    ls = '-' if t in ['Control', 'Neighbors'] else '--'\n",
    "    ax.plot(ages, pred_probs * 100, color=c, linewidth=lw, linestyle=ls, label=t)\n",
    "\n",
    "ax.set_xlabel('Age', fontsize=13)\n",
    "ax.set_ylabel('Predicted Turnout Probability (%)', fontsize=13)\n",
    "ax.set_title('Predicted Turnout by Age and Treatment\\n(for a voter who voted in past generals but not primaries)',\n",
    "             fontsize=13)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.set_ylim(0, 70)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Note the S-shaped curves: logistic regression naturally produces these.')\n",
    "print('The treatment gap (Control vs Neighbors) is roughly constant across ages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: The Bigger Picture (Coppock, Hill, Vavreck)\n",
    "\n",
    "GGL found that social pressure mailers can increase turnout by up to 8.1 percentage points. That's a huge effect by the standards of political science.\n",
    "\n",
    "How does this compare to **persuasion** effects in campaigns?\n",
    "\n",
    "Coppock, Hill, and Vavreck (2020) analyzed **59 real-time randomized experiments** on political advertising. Their finding: the average persuasion effect is **tiny**, around 0.5 percentage points.\n",
    "\n",
    "In their 2024 paper (with Hewitt et al.), they expanded this to **146 experiments from 51 campaigns**, finding similar results.\n",
    "\n",
    "Let's visualize this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: GGL turnout effects vs typical campaign persuasion effects\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# GGL effects (from our data)\n",
    "ggl_effects = {\n",
    "    'GGL: Civic Duty': 1.8,\n",
    "    'GGL: Hawthorne': 2.6,\n",
    "    'GGL: Self': 4.9,\n",
    "    'GGL: Neighbors': 8.1\n",
    "}\n",
    "\n",
    "# Coppock et al. typical effects (from their papers)\n",
    "persuasion_effects = {\n",
    "    'Typical TV ad\\n(Coppock+ 2020)': 0.5,\n",
    "    'Typical digital ad\\n(Hewitt+ 2024)': 0.4,\n",
    "    'Best campaign ad\\n(95th percentile)': 1.5,\n",
    "}\n",
    "\n",
    "all_effects = {**persuasion_effects, **ggl_effects}\n",
    "labels = list(all_effects.keys())\n",
    "values = list(all_effects.values())\n",
    "bar_colors = ['#9b59b6'] * len(persuasion_effects) + colors[1:]\n",
    "\n",
    "bars = ax.barh(labels, values, color=bar_colors, edgecolor='white', linewidth=1.5, height=0.6)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_width() + 0.15, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.1f} pp', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Effect Size (percentage points)', fontsize=13)\n",
    "ax.set_title('Social Pressure (GGL) vs Campaign Persuasion (Coppock+)\\nEffect sizes from randomized experiments',\n",
    "             fontsize=13)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 10)\n",
    "\n",
    "# Add dividing annotation\n",
    "ax.axhline(2.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(9.5, 4.5, 'Social pressure\\n(GGL 2008)', ha='right', fontsize=10,\n",
    "        fontstyle='italic', color='gray')\n",
    "ax.text(9.5, 0.8, 'Campaign persuasion\\n(Coppock+ 2020, 2024)', ha='right', fontsize=10,\n",
    "        fontstyle='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key takeaway from Coppock, Hill, and Vavreck:')\n",
    "print('Campaign persuasion effects are real but VERY small (about 0.5 pp).')\n",
    "print('GGL\\'s social pressure effects are 2x to 16x larger.')\n",
    "print('This suggests mobilization (getting people to show up) is easier than persuasion (changing minds).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the difference?\n",
    "\n",
    "**Mobilization** (GGL): Getting people who *already agree with you* to actually show up and vote. Social pressure is a powerful lever because it exploits accountability to neighbors.\n",
    "\n",
    "**Persuasion** (Coppock+): Changing people's minds about *which candidate to support*. Much harder. Most ads \"preach to the choir\" or are ignored.\n",
    "\n",
    "This distinction, between mobilization and persuasion, is central to modern campaign strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Logistic Regression Under the Hood\n",
    "\n",
    "Let's build more intuition about what the logistic model is doing, using a continuous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with age as a continuous predictor\n",
    "logit_age = smf.logit('voted_binary ~ age', data=df).fit(disp=0)\n",
    "\n",
    "print('Logistic regression: voted ~ age')\n",
    "print(f'  Intercept: {logit_age.params[\"Intercept\"]:.4f}')\n",
    "print(f'  Age coef:  {logit_age.params[\"age\"]:.4f}')\n",
    "print(f'\\nInterpretation: each additional year of age increases the log-odds')\n",
    "print(f'of voting by {logit_age.params[\"age\"]:.4f},')\n",
    "print(f'or multiplies the odds by {np.exp(logit_age.params[\"age\"]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: empirical turnout rates by age vs logistic fit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bin ages and compute empirical turnout rates\n",
    "df['age_bin'] = pd.cut(df['age'], bins=range(18, 92, 2))\n",
    "age_rates = df.groupby('age_bin')['voted_binary'].agg(['mean', 'count'])\n",
    "age_rates['midpoint'] = [interval.mid for interval in age_rates.index]\n",
    "age_rates = age_rates[age_rates['count'] > 100]  # drop tiny bins\n",
    "\n",
    "# Left: probability scale\n",
    "ax = axes[0]\n",
    "ax.scatter(age_rates['midpoint'], age_rates['mean'] * 100,\n",
    "           s=age_rates['count']/50, alpha=0.6, color='steelblue',\n",
    "           label='Empirical rate (size = N)')\n",
    "\n",
    "# Logistic fit\n",
    "age_pred = np.linspace(20, 90, 200)\n",
    "pred_df_age = pd.DataFrame({'age': age_pred})\n",
    "pred_probs_age = logit_age.predict(pred_df_age)\n",
    "ax.plot(age_pred, pred_probs_age * 100, 'r-', linewidth=2.5, label='Logistic fit')\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Turnout Rate (%)')\n",
    "ax.set_title('Probability Scale')\n",
    "ax.legend()\n",
    "\n",
    "# Right: log-odds scale\n",
    "ax = axes[1]\n",
    "empirical_log_odds = np.log(age_rates['mean'] / (1 - age_rates['mean']))\n",
    "ax.scatter(age_rates['midpoint'], empirical_log_odds,\n",
    "           s=age_rates['count']/50, alpha=0.6, color='steelblue',\n",
    "           label='Empirical log-odds')\n",
    "\n",
    "# On log-odds scale, the fit is a straight line!\n",
    "log_odds_pred = logit_age.params['Intercept'] + logit_age.params['age'] * age_pred\n",
    "ax.plot(age_pred, log_odds_pred, 'r-', linewidth=2.5, label='Logistic fit (linear!)')\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Log-odds of Voting')\n",
    "ax.set_title('Log-odds Scale')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Left: On the probability scale, the logistic fit is an S-curve.')\n",
    "print('Right: On the log-odds scale, it is a straight line.')\n",
    "print('This is the key insight: logistic regression is just linear regression on log-odds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Practice Exercises\n",
    "\n",
    "Try these on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Past Primary Voting\n",
    "\n",
    "People who voted in the 2004 primary (`p2004_bin`) are probably more likely to vote in the 2006 primary.\n",
    "\n",
    "1. Compute the turnout rate for people who did vs didn't vote in the 2004 primary\n",
    "2. Fit a logistic regression with `p2004_bin` as the only predictor\n",
    "3. What is the odds ratio? Interpret it in plain language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Hint: \n",
    "# df.groupby('p2004_bin')['voted_binary'].mean()\n",
    "# smf.logit('voted_binary ~ p2004_bin', data=df).fit(disp=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Interaction Effects\n",
    "\n",
    "Does the Neighbors treatment work differently for people who voted in the 2004 general election vs those who didn't?\n",
    "\n",
    "1. Create a subset of just the Control and Neighbors groups\n",
    "2. Fit a logistic regression with an interaction: `voted_binary ~ neighbors * g2004_bin`\n",
    "3. Is the interaction significant? What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Hint:\n",
    "# subset = df[df['treatment'].isin(['Control', 'Neighbors'])].copy()\n",
    "# subset['neighbors'] = (subset['treatment'] == 'Neighbors').astype(int)\n",
    "# smf.logit('voted_binary ~ neighbors * g2004_bin', data=subset).fit(disp=0).summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Household Clustering\n",
    "\n",
    "The GGL experiment randomized at the **household** level, not the individual level. People in the same household got the same treatment.\n",
    "\n",
    "Why might this matter for our standard errors? (Think about whether two people in the same household are likely to have correlated voting behavior.)\n",
    "\n",
    "For those who want to try: `statsmodels` supports clustered standard errors via the `cov_type='cluster'` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (optional / advanced)\n",
    "\n",
    "# Hint for clustered SEs with OLS:\n",
    "# lpm.get_robustcov_results(cov_type='cluster', groups=df['hh_id']).summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "print('Turnout by 2004 primary voting:')\n",
    "print(df.groupby('p2004_bin')['voted_binary'].mean())\n",
    "print()\n",
    "\n",
    "logit_p04 = smf.logit('voted_binary ~ p2004_bin', data=df).fit(disp=0)\n",
    "print(f'Log-odds coefficient: {logit_p04.params[\"p2004_bin\"]:.4f}')\n",
    "print(f'Odds ratio: {np.exp(logit_p04.params[\"p2004_bin\"]):.2f}')\n",
    "print()\n",
    "print('Interpretation: People who voted in the 2004 primary had')\n",
    "print(f'{np.exp(logit_p04.params[\"p2004_bin\"]):.1f}x the odds of voting in the 2006 primary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "subset = df[df['treatment'].isin(['Control', 'Neighbors'])].copy()\n",
    "subset['neighbors'] = (subset['treatment'] == 'Neighbors').astype(int)\n",
    "\n",
    "logit_interact = smf.logit('voted_binary ~ neighbors * g2004_bin', data=subset).fit(disp=0)\n",
    "print(logit_interact.summary().tables[1])\n",
    "print()\n",
    "print('The interaction term tells us whether the Neighbors effect differs')\n",
    "print('for 2004 general election voters vs non-voters.')\n",
    "print('A negative interaction would mean the treatment is LESS effective for past voters')\n",
    "print('(ceiling effect: they were already likely to vote).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **GGL (2008)** ran a massive field experiment showing social pressure increases voter turnout, with the Neighbors treatment producing an 8.1 pp effect\n",
    "\n",
    "2. **Simple difference-in-means** is the right starting point for RCT analysis\n",
    "\n",
    "3. **Logistic regression** is a tool for binary outcomes that:\n",
    "   - Keeps predictions in [0, 1]\n",
    "   - Models log-odds as a linear function\n",
    "   - Reports odds ratios (how much the odds multiply)\n",
    "   - Produces S-shaped predicted probability curves\n",
    "\n",
    "4. **Adding covariates** to an RCT improves precision without changing the point estimate\n",
    "\n",
    "5. **Coppock, Hill, and Vavreck (2020/2024)** show that campaign *persuasion* effects are much smaller (~0.5 pp) than GGL's *mobilization* effects\n",
    "\n",
    "**Connection to the course:** The methods in this notebook (randomized experiments, regression, logistic regression) are the foundations of measuring whether persuasion works. The GGL experiment is a clean example of how randomization lets us draw causal conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
