{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Pressure, Voter Turnout, and Logistic Regression\n",
    "\n",
    "**Persuasion at Scale** | Week 4, Lecture 8 | 2026-02-16\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chrishwiggins/social-pressure-voting/blob/main/notebooks/social-pressure-logistic-regression.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks through the data from two assigned readings:\n",
    "\n",
    "1. **Gerber, Green, and Larimer (2008)** \"Social Pressure and Voter Turnout\" (APSR)\n",
    "2. **Coppock, Hill, and Vavreck (2020/2024)** on campaign persuasion effects\n",
    "\n",
    "We will:\n",
    "- Load the **actual experimental data** from GGL (2008): 344,084 individuals, five treatment groups\n",
    "- Reproduce the paper's main results\n",
    "- Introduce **logistic regression** as the right tool for binary outcomes\n",
    "- Build intuition visually before touching any math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 13\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Experiment\n",
    "\n",
    "In August 2006, Gerber, Green, and Larimer ran a field experiment with **344,084 registered voters** in Michigan. Households were randomly assigned to one of five groups:\n",
    "\n",
    "| Group | What they received | N |\n",
    "|-------|-------------------|---|\n",
    "| **Control** | Nothing | 191,243 |\n",
    "| **Civic Duty** | \"DO YOUR CIVIC DUTY - VOTE!\" | 38,218 |\n",
    "| **Hawthorne** | \"YOU ARE BEING STUDIED\" (told researchers would check if they voted) | 38,204 |\n",
    "| **Self** | Showed the recipient's own past voting record | 38,218 |\n",
    "| **Neighbors** | Showed the recipient AND their neighbors' past voting records | 38,201 |\n",
    "\n",
    "The key outcome: **did they vote in the August 2006 primary?** (binary: yes/no)\n",
    "\n",
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the actual replication data from ISPS Yale\n",
    "url = 'http://hdl.handle.net/10079/d3669799-4537-411e-b175-d9e837324c35'\n",
    "df = pd.read_csv(url)\n",
    "print(f'Loaded {len(df):,} individual records')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "df['treatment'] = df['treatment'].str.strip()\n",
    "df['voted_binary'] = (df['voted'] == 'Yes').astype(int)\n",
    "df['age'] = 2006 - df['yob']  # approximate age at time of election\n",
    "\n",
    "# Past voting: convert yes/no to 1/0 for covariates\n",
    "for col in ['g2000', 'g2002', 'g2004', 'p2000', 'p2002', 'p2004']:\n",
    "    df[col + '_bin'] = (df[col].str.strip().str.lower() == 'yes').astype(int)\n",
    "\n",
    "print(f'Age range: {df[\"age\"].min()} to {df[\"age\"].max()}')\n",
    "print(f'Overall turnout: {df[\"voted_binary\"].mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "\n",
    "Each row is one registered voter. The key columns:\n",
    "- `treatment`: which experimental group they were assigned to\n",
    "- `voted`: did they vote in the August 2006 primary? (our **outcome**)\n",
    "- `g2000`, `g2002`, `g2004`: voted in general elections in 2000, 2002, 2004?\n",
    "- `p2000`, `p2002`, `p2004`: voted in primary elections in 2000, 2002, 2004?\n",
    "- `yob`: year of birth\n",
    "- `hh_id`, `hh_size`: household ID and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment group sizes\n",
    "group_order = ['Control', 'Civic Duty', 'Hawthorne', 'Self', 'Neighbors']\n",
    "colors = ['#95a5a6', '#3498db', '#2ecc71', '#e67e22', '#e74c3c']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "counts = [len(df[df['treatment'] == t]) for t in group_order]\n",
    "bars = ax.bar(group_order, counts, color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2000,\n",
    "            f'{count:,}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Number of Individuals')\n",
    "ax.set_title('Sample Sizes by Treatment Group')\n",
    "ax.set_ylim(0, 220000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the control group is about 5x larger than each treatment group. This is a deliberate design choice: it's cheap to not send mail, so the researchers allocated more people to control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Main Result (Difference in Means)\n",
    "\n",
    "The simplest analysis of an RCT: compare the average outcome across groups.\n",
    "\n",
    "Because treatment was **randomly assigned**, any difference in turnout between groups is a valid estimate of the **causal effect** of the mailer.\n",
    "\n",
    "This is the **intent-to-treat (ITT) effect**: the effect of being *assigned* the treatment (not necessarily reading the mailer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce Table 2 from the paper\n",
    "control_rate = df[df['treatment'] == 'Control']['voted_binary'].mean()\n",
    "\n",
    "print('=' * 60)\n",
    "print('Replicating GGL (2008), Table 2')\n",
    "print('=' * 60)\n",
    "print(f'{\"Group\":15s} {\"Turnout\":>10s} {\"ITT Effect\":>12s} {\"N\":>10s}')\n",
    "print('-' * 60)\n",
    "\n",
    "for t in group_order:\n",
    "    sub = df[df['treatment'] == t]\n",
    "    rate = sub['voted_binary'].mean()\n",
    "    effect = (rate - control_rate) * 100 if t != 'Control' else 0\n",
    "    effect_str = f'{effect:+.1f} pp' if t != 'Control' else '(baseline)'\n",
    "    print(f'{t:15s} {rate*100:9.1f}% {effect_str:>12s} {len(sub):>10,}')\n",
    "\n",
    "print('-' * 60)\n",
    "print(f'\\nNeighbors effect: +8.1 percentage points over control')\n",
    "print(f'That is a {8.1/29.7*100:.0f}% relative increase in turnout!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: turnout by treatment group\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rates = [df[df['treatment'] == t]['voted_binary'].mean() * 100 for t in group_order]\n",
    "bars = ax.bar(group_order, rates, color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "# Add rate labels\n",
    "for bar, rate in zip(bars, rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add horizontal line at control rate\n",
    "ax.axhline(y=rates[0], color='gray', linestyle='--', alpha=0.7, label=f'Control baseline ({rates[0]:.1f}%)')\n",
    "\n",
    "ax.set_ylabel('Turnout Rate (%)')\n",
    "ax.set_title('Voter Turnout by Treatment Group\\nGerber, Green, and Larimer (2008)', fontsize=14)\n",
    "ax.set_ylim(25, 42)\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Neighbors\" treatment, which showed recipients their neighbors' past voting records and promised to send an updated version after the election, produced the largest effect: **+8.1 percentage points** over the control group.\n",
    "\n",
    "The treatments are ordered by social pressure intensity:\n",
    "- Civic Duty (weakest): generic civic appeal\n",
    "- Hawthorne: you're being watched\n",
    "- Self: we know YOUR voting history\n",
    "- Neighbors (strongest): we know your neighbors' history too, and we'll tell them yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking randomization\n",
    "\n",
    "One advantage of the real data: we can verify that randomization \"worked.\" If treatment was truly random, pre-treatment covariates should be balanced across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance check: past voting history by treatment group\n",
    "balance_vars = ['g2004_bin', 'g2002_bin', 'g2000_bin', 'p2004_bin', 'age']\n",
    "balance_labels = ['Voted 2004 Gen', 'Voted 2002 Gen', 'Voted 2000 Gen',\n",
    "                  'Voted 2004 Primary', 'Age']\n",
    "\n",
    "print(f'{\"Variable\":20s}', end='')\n",
    "for t in group_order:\n",
    "    print(f'{t:>12s}', end='')\n",
    "print()\n",
    "print('-' * 80)\n",
    "\n",
    "for var, label in zip(balance_vars, balance_labels):\n",
    "    print(f'{label:20s}', end='')\n",
    "    for t in group_order:\n",
    "        val = df[df['treatment'] == t][var].mean()\n",
    "        if var == 'age':\n",
    "            print(f'{val:12.1f}', end='')\n",
    "        else:\n",
    "            print(f'{val*100:11.1f}%', end='')\n",
    "    print()\n",
    "\n",
    "print('\\nVery similar across groups = randomization worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual balance check: age distribution by group\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4), sharey=True)\n",
    "\n",
    "for ax, t, c in zip(axes, group_order, colors):\n",
    "    sub = df[df['treatment'] == t]\n",
    "    ax.hist(sub['age'], bins=range(18, 100, 2), color=c, alpha=0.8, density=True)\n",
    "    ax.set_title(t, fontsize=11)\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.axvline(sub['age'].mean(), color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "axes[0].set_ylabel('Density')\n",
    "fig.suptitle('Age Distributions Look Identical Across Groups (Randomization Worked)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Why Not Just Stop Here?\n",
    "\n",
    "The difference-in-means analysis is perfectly valid for an RCT. So why bother with regression at all?\n",
    "\n",
    "Three reasons:\n",
    "1. **Precision**: Adding pre-treatment covariates can reduce noise and give tighter confidence intervals\n",
    "2. **Subgroup effects**: We might want to know if the treatment works differently for different people\n",
    "3. **The outcome is binary**: Turnout is either 0 or 1. Regular linear regression can predict values outside [0, 1]. Logistic regression respects the binary nature of the outcome.\n",
    "\n",
    "Let's see why #3 matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with the linear probability model\n",
    "\n",
    "A **linear probability model (LPM)** just runs ordinary regression on a binary outcome:\n",
    "\n",
    "$$\\text{voted}_i = \\beta_0 + \\beta_1 \\cdot \\text{treatment}_i + \\varepsilon_i$$\n",
    "\n",
    "This gives us an easy-to-interpret coefficient: $\\beta_1$ is the change in probability of voting.\n",
    "\n",
    "But it has a conceptual problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example to illustrate the problem\n",
    "# Turnout vs age: older people vote more\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Take a random sample for plotting (full dataset is too dense)\n",
    "sample = df.sample(3000, random_state=42)\n",
    "\n",
    "# Left: scatter of actual data with linear fit\n",
    "ax = axes[0]\n",
    "jitter = np.random.uniform(-0.05, 0.05, len(sample))\n",
    "ax.scatter(sample['age'], sample['voted_binary'] + jitter,\n",
    "           alpha=0.1, s=10, color='steelblue')\n",
    "\n",
    "# Linear fit\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "age_range = np.linspace(18, 95, 200)\n",
    "b, m = polyfit(sample['age'], sample['voted_binary'], 1)\n",
    "ax.plot(age_range, b + m * age_range, 'r-', linewidth=2.5, label='Linear fit')\n",
    "\n",
    "ax.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(1, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.fill_between(age_range, -0.3, 0, alpha=0.1, color='red')\n",
    "ax.fill_between(age_range, 1, 1.3, alpha=0.1, color='red')\n",
    "ax.text(80, -0.15, 'Impossible!\\n(probability < 0)', color='red', fontsize=10, ha='center')\n",
    "ax.text(80, 1.1, 'Impossible!\\n(probability > 1)', color='red', fontsize=10, ha='center')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Voted (0/1)')\n",
    "ax.set_title('Linear Probability Model\\n(can predict outside [0, 1])')\n",
    "ax.set_ylim(-0.3, 1.3)\n",
    "ax.legend()\n",
    "\n",
    "# Right: the logistic (sigmoid) function\n",
    "ax = axes[1]\n",
    "z = np.linspace(-6, 6, 200)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "ax.plot(z, sigmoid, 'b-', linewidth=3)\n",
    "ax.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(1, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.fill_between(z, 0, 1, alpha=0.05, color='green')\n",
    "ax.text(0, 0.5, '  always between 0 and 1', color='green', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Linear predictor (z)')\n",
    "ax.set_ylabel('Predicted probability')\n",
    "ax.set_title('Logistic (Sigmoid) Function\\n$\\sigma(z) = 1 / (1 + e^{-z})$')\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot shows the problem: a straight line through binary (0/1) data will eventually predict probabilities below 0 or above 1.\n",
    "\n",
    "The right plot shows the solution: the **logistic function** (also called the sigmoid) squashes any input into the range (0, 1). It's an S-shaped curve that:\n",
    "- Approaches 0 as the input goes to $-\\infty$\n",
    "- Approaches 1 as the input goes to $+\\infty$\n",
    "- Equals exactly 0.5 when the input is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: What Logistic Regression Actually Does\n",
    "\n",
    "Instead of modeling probability directly:\n",
    "\n",
    "$$P(\\text{voted}=1) = \\beta_0 + \\beta_1 x \\quad \\text{(linear, can go outside [0,1])}$$\n",
    "\n",
    "Logistic regression models the **log-odds**:\n",
    "\n",
    "$$\\log\\left(\\frac{P(\\text{voted}=1)}{1 - P(\\text{voted}=1)}\\right) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "Equivalently, the predicted probability is:\n",
    "\n",
    "$$P(\\text{voted}=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
    "\n",
    "Let's build intuition for what \"odds\" and \"log-odds\" mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: probability -> odds -> log-odds\n",
    "probs = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "odds = probs / (1 - probs)\n",
    "log_odds = np.log(odds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "# Probability scale\n",
    "ax = axes[0]\n",
    "ax.barh(range(len(probs)), probs, color='steelblue', height=0.6)\n",
    "ax.set_yticks(range(len(probs)))\n",
    "ax.set_yticklabels([f'{p:.0%}' for p in probs])\n",
    "ax.set_xlabel('Probability')\n",
    "ax.set_title('Probability\\n(bounded 0 to 1)')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Odds scale\n",
    "ax = axes[1]\n",
    "ax.barh(range(len(probs)), odds, color='#e67e22', height=0.6)\n",
    "ax.set_yticks(range(len(probs)))\n",
    "ax.set_yticklabels([f'{o:.2f}' for o in odds])\n",
    "ax.set_xlabel('Odds')\n",
    "ax.set_title('Odds = p/(1-p)\\n(bounded 0 to $\\infty$)')\n",
    "\n",
    "# Log-odds scale\n",
    "ax = axes[2]\n",
    "bar_colors = ['#e74c3c' if lo < 0 else '#2ecc71' for lo in log_odds]\n",
    "ax.barh(range(len(probs)), log_odds, color=bar_colors, height=0.6)\n",
    "ax.set_yticks(range(len(probs)))\n",
    "ax.set_yticklabels([f'{lo:+.2f}' for lo in log_odds])\n",
    "ax.set_xlabel('Log-odds')\n",
    "ax.set_title('Log-odds = log(p/(1-p))\\n(unbounded: $-\\infty$ to $+\\infty$)')\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key insight: log-odds is the scale where logistic regression is linear.')\n",
    "print('Positive log-odds = more likely than not (p > 50%)')\n",
    "print('Negative log-odds = less likely than not (p < 50%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why log-odds?** It's the transformation that maps probabilities (bounded between 0 and 1) to an unbounded scale ($-\\infty$ to $+\\infty$). On the log-odds scale, it makes sense to fit a linear model.\n",
    "\n",
    "Think of it this way:\n",
    "- A probability of 50% = odds of 1:1 = log-odds of 0\n",
    "- A probability of 90% = odds of 9:1 = log-odds of +2.2\n",
    "- A probability of 10% = odds of 1:9 = log-odds of -2.2\n",
    "\n",
    "The symmetry is nice: 90% and 10% are equally far from 50%, just in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Logistic Regression on the GGL Data\n",
    "\n",
    "Now let's fit logistic regression to the real experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import statsmodels.formula.api as smf\n\n# First: the simple linear probability model (OLS) for comparison\nlpm = smf.ols('voted_binary ~ C(treatment, Treatment(reference=\"Control\"))', data=df).fit()\n\nprint('=' * 65)\nprint('LINEAR PROBABILITY MODEL (OLS)')\nprint('Coefficients are changes in probability of voting')\nprint('=' * 65)\nfor name, coef in lpm.params.items():\n    if 'Intercept' in name:\n        print(f'  Control baseline:  {coef:.4f} ({coef*100:.1f}%)')\n    else:\n        # Extract treatment name from e.g. C(treatment, Treatment(reference=\"Control\"))[T.Civic Duty]\n        tname = name.split('[T.')[1].rstrip(']')\n        print(f'  {tname:15s}:  {coef:+.4f} ({coef*100:+.1f} pp)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now: logistic regression\nlogit = smf.logit('voted_binary ~ C(treatment, Treatment(reference=\"Control\"))', data=df).fit(disp=0)\n\nprint('=' * 65)\nprint('LOGISTIC REGRESSION')\nprint('Coefficients are changes in LOG-ODDS of voting')\nprint('=' * 65)\nfor name, coef in logit.params.items():\n    if 'Intercept' in name:\n        p = np.exp(coef) / (1 + np.exp(coef))\n        print(f'  Control (intercept): {coef:.4f} (log-odds)  ->  {p*100:.1f}% (probability)')\n    else:\n        tname = name.split('[T.')[1].rstrip(']')\n        or_val = np.exp(coef)\n        print(f'  {tname:15s}: {coef:+.4f} (log-odds)  odds ratio: {or_val:.3f}')\n\nprint()\nprint('An odds ratio > 1 means the treatment INCREASES the odds of voting.')\nprint('For example, OR=1.45 means 45% higher odds of voting than control.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LPM and Logistic side by side\n",
    "treatment_names = ['Civic Duty', 'Hawthorne', 'Self', 'Neighbors']\n",
    "\n",
    "# Get marginal effects from logistic (average predicted probability differences)\n",
    "logit_margins = logit.get_margeff()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: LPM coefficients (= difference in means)\n",
    "ax = axes[0]\n",
    "lpm_effects = [lpm.params[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'] * 100\n",
    "               for t in treatment_names]\n",
    "lpm_ci = [lpm.conf_int().loc[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'] * 100\n",
    "          for t in treatment_names]\n",
    "lpm_errors = [(e[1] - e[0])/2 for e in lpm_ci]\n",
    "\n",
    "ax.barh(treatment_names, lpm_effects, xerr=lpm_errors,\n",
    "        color=colors[1:], edgecolor='white', linewidth=1.5, capsize=5)\n",
    "ax.set_xlabel('Effect on Turnout (percentage points)')\n",
    "ax.set_title('Linear Probability Model\\n(coefficient = pp change)')\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Right: Logistic regression (odds ratios)\n",
    "ax = axes[1]\n",
    "odds_ratios = [np.exp(logit.params[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'])\n",
    "               for t in treatment_names]\n",
    "logit_ci_vals = logit.conf_int()\n",
    "or_lower = [np.exp(logit_ci_vals.loc[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]', 0])\n",
    "            for t in treatment_names]\n",
    "or_upper = [np.exp(logit_ci_vals.loc[f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]', 1])\n",
    "            for t in treatment_names]\n",
    "or_errors = [[o - l for o, l in zip(odds_ratios, or_lower)],\n",
    "             [u - o for o, u in zip(odds_ratios, or_upper)]]\n",
    "\n",
    "ax.barh(treatment_names, odds_ratios, xerr=or_errors,\n",
    "        color=colors[1:], edgecolor='white', linewidth=1.5, capsize=5)\n",
    "ax.set_xlabel('Odds Ratio (vs Control)')\n",
    "ax.set_title('Logistic Regression\\n(odds ratio: >1 means higher turnout)')\n",
    "ax.axvline(1, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Both models tell the same story: more social pressure -> more turnout.')\n",
    "print('The ranking is identical; only the scale of the coefficients differs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the numbers so similar?\n",
    "\n",
    "When the baseline probability is near 30% and the effects are modest (a few percentage points), the linear probability model and logistic regression give nearly identical answers.\n",
    "\n",
    "The logistic model matters more when:\n",
    "- Baseline rates are very high or very low (near 0% or 100%)\n",
    "- You have continuous covariates (like age) where the linear model might predict outside [0, 1]\n",
    "- You want predicted probabilities for new individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Adding Covariates\n",
    "\n",
    "In an RCT, adding pre-treatment covariates doesn't change the *bias* of our estimate (randomization already handles that), but it can improve **precision**.\n",
    "\n",
    "Past voting behavior is the strongest predictor of future voting. Let's add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with covariates\n",
    "logit_full = smf.logit(\n",
    "    'voted_binary ~ C(treatment, Treatment(reference=\"Control\"))'\n",
    "    ' + g2000_bin + g2002_bin + g2004_bin'\n",
    "    ' + p2000_bin + p2002_bin + p2004_bin'\n",
    "    ' + age + hh_size',\n",
    "    data=df\n",
    ").fit(disp=0)\n",
    "\n",
    "print('=' * 65)\n",
    "print('LOGISTIC REGRESSION WITH COVARIATES')\n",
    "print('=' * 65)\n",
    "print(logit_full.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confidence intervals: with and without covariates\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "y_positions = np.arange(len(treatment_names))\n",
    "offset = 0.15\n",
    "\n",
    "for i, t in enumerate(treatment_names):\n",
    "    key = f'C(treatment, Treatment(reference=\"Control\"))[T.{t}]'\n",
    "\n",
    "    # Without covariates\n",
    "    coef1 = logit.params[key]\n",
    "    ci1 = logit.conf_int().loc[key]\n",
    "    ax.plot([ci1[0], ci1[1]], [i + offset, i + offset], 'o-',\n",
    "            color='steelblue', linewidth=2.5, markersize=6)\n",
    "\n",
    "    # With covariates\n",
    "    coef2 = logit_full.params[key]\n",
    "    ci2 = logit_full.conf_int().loc[key]\n",
    "    ax.plot([ci2[0], ci2[1]], [i - offset, i - offset], 's-',\n",
    "            color='#e74c3c', linewidth=2.5, markersize=6)\n",
    "\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(treatment_names)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('Log-odds Coefficient (with 95% CI)')\n",
    "ax.set_title('Treatment Effects: With vs Without Covariates')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='steelblue', label='Treatment only')\n",
    "red_patch = mpatches.Patch(color='#e74c3c', label='+ voting history, age, hh_size')\n",
    "ax.legend(handles=[blue_patch, red_patch], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Red intervals are narrower: covariates absorb noise, giving more precise estimates.')\n",
    "print('The point estimates barely change (because randomization already removed bias).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Predicted Probabilities\n",
    "\n",
    "One of the nicest features of logistic regression: we can compute predicted probabilities for specific types of voters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted turnout by age and treatment group\n",
    "ages = np.arange(20, 90)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for t, c in zip(group_order, colors):\n",
    "    # Create prediction data: median voter characteristics, varying age\n",
    "    pred_df = pd.DataFrame({\n",
    "        'treatment': t,\n",
    "        'g2000_bin': 1,  # typical voter: voted in past generals\n",
    "        'g2002_bin': 1,\n",
    "        'g2004_bin': 1,\n",
    "        'p2000_bin': 0,  # but not in past primaries\n",
    "        'p2002_bin': 0,\n",
    "        'p2004_bin': 0,\n",
    "        'age': ages,\n",
    "        'hh_size': 2\n",
    "    })\n",
    "    pred_probs = logit_full.predict(pred_df)\n",
    "    lw = 3 if t in ['Control', 'Neighbors'] else 1.5\n",
    "    ls = '-' if t in ['Control', 'Neighbors'] else '--'\n",
    "    ax.plot(ages, pred_probs * 100, color=c, linewidth=lw, linestyle=ls, label=t)\n",
    "\n",
    "ax.set_xlabel('Age', fontsize=13)\n",
    "ax.set_ylabel('Predicted Turnout Probability (%)', fontsize=13)\n",
    "ax.set_title('Predicted Turnout by Age and Treatment\\n(for a voter who voted in past generals but not primaries)',\n",
    "             fontsize=13)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.set_ylim(0, 70)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Note the S-shaped curves: logistic regression naturally produces these.')\n",
    "print('The treatment gap (Control vs Neighbors) is roughly constant across ages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: The Bigger Picture (Coppock, Hill, Vavreck)\n",
    "\n",
    "GGL found that social pressure mailers can increase turnout by up to 8.1 percentage points. That's a huge effect by the standards of political science.\n",
    "\n",
    "How does this compare to **persuasion** effects in campaigns?\n",
    "\n",
    "Coppock, Hill, and Vavreck (2020) analyzed **59 real-time randomized experiments** on political advertising. Their finding: the average persuasion effect is **tiny**, around 0.5 percentage points.\n",
    "\n",
    "In their 2024 paper (with Hewitt et al.), they expanded this to **146 experiments from 51 campaigns**, finding similar results.\n",
    "\n",
    "Let's visualize this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: GGL turnout effects vs typical campaign persuasion effects\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# GGL effects (from our data)\n",
    "ggl_effects = {\n",
    "    'GGL: Civic Duty': 1.8,\n",
    "    'GGL: Hawthorne': 2.6,\n",
    "    'GGL: Self': 4.9,\n",
    "    'GGL: Neighbors': 8.1\n",
    "}\n",
    "\n",
    "# Coppock et al. typical effects (from their papers)\n",
    "persuasion_effects = {\n",
    "    'Typical TV ad\\n(Coppock+ 2020)': 0.5,\n",
    "    'Typical digital ad\\n(Hewitt+ 2024)': 0.4,\n",
    "    'Best campaign ad\\n(95th percentile)': 1.5,\n",
    "}\n",
    "\n",
    "all_effects = {**persuasion_effects, **ggl_effects}\n",
    "labels = list(all_effects.keys())\n",
    "values = list(all_effects.values())\n",
    "bar_colors = ['#9b59b6'] * len(persuasion_effects) + colors[1:]\n",
    "\n",
    "bars = ax.barh(labels, values, color=bar_colors, edgecolor='white', linewidth=1.5, height=0.6)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_width() + 0.15, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.1f} pp', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Effect Size (percentage points)', fontsize=13)\n",
    "ax.set_title('Social Pressure (GGL) vs Campaign Persuasion (Coppock+)\\nEffect sizes from randomized experiments',\n",
    "             fontsize=13)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 10)\n",
    "\n",
    "# Add dividing annotation\n",
    "ax.axhline(2.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(9.5, 4.5, 'Social pressure\\n(GGL 2008)', ha='right', fontsize=10,\n",
    "        fontstyle='italic', color='gray')\n",
    "ax.text(9.5, 0.8, 'Campaign persuasion\\n(Coppock+ 2020, 2024)', ha='right', fontsize=10,\n",
    "        fontstyle='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key takeaway from Coppock, Hill, and Vavreck:')\n",
    "print('Campaign persuasion effects are real but VERY small (about 0.5 pp).')\n",
    "print('GGL\\'s social pressure effects are 2x to 16x larger.')\n",
    "print('This suggests mobilization (getting people to show up) is easier than persuasion (changing minds).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the difference?\n",
    "\n",
    "**Mobilization** (GGL): Getting people who *already agree with you* to actually show up and vote. Social pressure is a powerful lever because it exploits accountability to neighbors.\n",
    "\n",
    "**Persuasion** (Coppock+): Changing people's minds about *which candidate to support*. Much harder. Most ads \"preach to the choir\" or are ignored.\n",
    "\n",
    "This distinction, between mobilization and persuasion, is central to modern campaign strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Logistic Regression Under the Hood\n",
    "\n",
    "Let's build more intuition about what the logistic model is doing, using a continuous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with age as a continuous predictor\n",
    "logit_age = smf.logit('voted_binary ~ age', data=df).fit(disp=0)\n",
    "\n",
    "print('Logistic regression: voted ~ age')\n",
    "print(f'  Intercept: {logit_age.params[\"Intercept\"]:.4f}')\n",
    "print(f'  Age coef:  {logit_age.params[\"age\"]:.4f}')\n",
    "print(f'\\nInterpretation: each additional year of age increases the log-odds')\n",
    "print(f'of voting by {logit_age.params[\"age\"]:.4f},')\n",
    "print(f'or multiplies the odds by {np.exp(logit_age.params[\"age\"]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: empirical turnout rates by age vs logistic fit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bin ages and compute empirical turnout rates\n",
    "df['age_bin'] = pd.cut(df['age'], bins=range(18, 92, 2))\n",
    "age_rates = df.groupby('age_bin')['voted_binary'].agg(['mean', 'count'])\n",
    "age_rates['midpoint'] = [interval.mid for interval in age_rates.index]\n",
    "age_rates = age_rates[age_rates['count'] > 100]  # drop tiny bins\n",
    "\n",
    "# Left: probability scale\n",
    "ax = axes[0]\n",
    "ax.scatter(age_rates['midpoint'], age_rates['mean'] * 100,\n",
    "           s=age_rates['count']/50, alpha=0.6, color='steelblue',\n",
    "           label='Empirical rate (size = N)')\n",
    "\n",
    "# Logistic fit\n",
    "age_pred = np.linspace(20, 90, 200)\n",
    "pred_df_age = pd.DataFrame({'age': age_pred})\n",
    "pred_probs_age = logit_age.predict(pred_df_age)\n",
    "ax.plot(age_pred, pred_probs_age * 100, 'r-', linewidth=2.5, label='Logistic fit')\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Turnout Rate (%)')\n",
    "ax.set_title('Probability Scale')\n",
    "ax.legend()\n",
    "\n",
    "# Right: log-odds scale\n",
    "ax = axes[1]\n",
    "empirical_log_odds = np.log(age_rates['mean'] / (1 - age_rates['mean']))\n",
    "ax.scatter(age_rates['midpoint'], empirical_log_odds,\n",
    "           s=age_rates['count']/50, alpha=0.6, color='steelblue',\n",
    "           label='Empirical log-odds')\n",
    "\n",
    "# On log-odds scale, the fit is a straight line!\n",
    "log_odds_pred = logit_age.params['Intercept'] + logit_age.params['age'] * age_pred\n",
    "ax.plot(age_pred, log_odds_pred, 'r-', linewidth=2.5, label='Logistic fit (linear!)')\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Log-odds of Voting')\n",
    "ax.set_title('Log-odds Scale')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Left: On the probability scale, the logistic fit is an S-curve.')\n",
    "print('Right: On the log-odds scale, it is a straight line.')\n",
    "print('This is the key insight: logistic regression is just linear regression on log-odds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Practice Exercises\n",
    "\n",
    "Try these on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Past Primary Voting\n",
    "\n",
    "People who voted in the 2004 primary (`p2004_bin`) are probably more likely to vote in the 2006 primary.\n",
    "\n",
    "1. Compute the turnout rate for people who did vs didn't vote in the 2004 primary\n",
    "2. Fit a logistic regression with `p2004_bin` as the only predictor\n",
    "3. What is the odds ratio? Interpret it in plain language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Hint: \n",
    "# df.groupby('p2004_bin')['voted_binary'].mean()\n",
    "# smf.logit('voted_binary ~ p2004_bin', data=df).fit(disp=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Interaction Effects\n\nDoes the Neighbors treatment work differently for people who voted in the 2004 *primary* vs those who didn't?\n\n1. Create a subset of just the Control and Neighbors groups\n2. Fit a logistic regression with an interaction: `voted_binary ~ neighbors * p2004_bin`\n3. Is the interaction significant? What does it mean?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n\n# Hint:\n# subset = df[df['treatment'].isin(['Control', 'Neighbors'])].copy()\n# subset['neighbors'] = (subset['treatment'] == 'Neighbors').astype(int)\n# smf.logit('voted_binary ~ neighbors * p2004_bin', data=subset).fit(disp=0).summary()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Household Clustering\n",
    "\n",
    "The GGL experiment randomized at the **household** level, not the individual level. People in the same household got the same treatment.\n",
    "\n",
    "Why might this matter for our standard errors? (Think about whether two people in the same household are likely to have correlated voting behavior.)\n",
    "\n",
    "For those who want to try: `statsmodels` supports clustered standard errors via the `cov_type='cluster'` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (optional / advanced)\n",
    "\n",
    "# Hint for clustered SEs with OLS:\n",
    "# lpm.get_robustcov_results(cov_type='cluster', groups=df['hh_id']).summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "print('Turnout by 2004 primary voting:')\n",
    "print(df.groupby('p2004_bin')['voted_binary'].mean())\n",
    "print()\n",
    "\n",
    "logit_p04 = smf.logit('voted_binary ~ p2004_bin', data=df).fit(disp=0)\n",
    "print(f'Log-odds coefficient: {logit_p04.params[\"p2004_bin\"]:.4f}')\n",
    "print(f'Odds ratio: {np.exp(logit_p04.params[\"p2004_bin\"]):.2f}')\n",
    "print()\n",
    "print('Interpretation: People who voted in the 2004 primary had')\n",
    "print(f'{np.exp(logit_p04.params[\"p2004_bin\"]):.1f}x the odds of voting in the 2006 primary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2 Solution\nsubset = df[df['treatment'].isin(['Control', 'Neighbors'])].copy()\nsubset['neighbors'] = (subset['treatment'] == 'Neighbors').astype(int)\n\nlogit_interact = smf.logit('voted_binary ~ neighbors * p2004_bin', data=subset).fit(disp=0)\nprint(logit_interact.summary().tables[1])\nprint()\nprint('The interaction term tells us whether the Neighbors effect differs')\nprint('for 2004 primary voters vs non-voters.')\nprint('A negative interaction would mean the treatment is LESS effective for past primary voters')\nprint('(ceiling effect: they were already likely to vote).')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary (Parts 1-10)\n\n**What we learned:**\n\n1. **GGL (2008)** ran a massive field experiment showing social pressure increases voter turnout, with the Neighbors treatment producing an 8.1 pp effect\n\n2. **Simple difference-in-means** is the right starting point for RCT analysis\n\n3. **Logistic regression** is a tool for binary outcomes that:\n   - Keeps predictions in [0, 1]\n   - Models log-odds as a linear function\n   - Reports odds ratios (how much the odds multiply)\n   - Produces S-shaped predicted probability curves\n\n4. **Adding covariates** to an RCT improves precision without changing the point estimate\n\n5. **Coppock, Hill, and Vavreck (2020/2024)** show that campaign *persuasion* effects are much smaller (~0.5 pp) than GGL's *mobilization* effects\n\n**Connection to the course:** The methods in this notebook (randomized experiments, regression, logistic regression) are the foundations of measuring whether persuasion works. The GGL experiment is a clean example of how randomization lets us draw causal conclusions.\n\n**Part 11 (below)** goes further: once you have a model, you can *optimize* treatment assignment, moving from \"what is the average effect?\" to \"who should get which mailer?\""
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 11: Optimal Treatment Assignment (What GGL Left on the Table)\n\nGGL estimated the **average** treatment effect. But we can do better: assign each person the treatment that maximizes *their* probability of voting.\n\nThe idea:\n1. Fit a model for $P(\\text{vote} \\mid A=a, X=x)$ with treatment-covariate interactions\n2. For each person, compute $h(x) = \\arg\\max_a \\hat{P}(\\text{vote} \\mid a, x)$\n3. Estimate the value of this policy using the **Hajek estimator** (inverse propensity weighting)\n4. Learn an interpretable decision tree that approximates $h(x)$\n\nThis is an RCT, so the propensities $e(a) = P(A=a)$ are known from the randomization design. The Hajek estimator for a deterministic policy $\\pi(a \\mid x) = \\mathbf{1}[a = h(x)]$ is:\n\n$$\\hat{E}_\\pi[Y] = \\frac{\\sum_i Y_i \\cdot \\mathbf{1}[A_i = h(X_i)] / e(A_i)}{\\sum_i \\mathbf{1}[A_i = h(X_i)] / e(A_i)}$$\n\nWe only use observations where the person *happened* to receive the treatment our policy would assign them, reweighted by $1/e(a)$ to correct for unequal randomization probabilities.\n\n**Why didn't GGL do this?** The paper is from 2008. The optimal policy / uplift modeling literature (Athey & Imbens causal forests, personalized treatment rules) emerged around 2016. This is routine in ML but was absent from political science at the time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 1: Fit a model with treatment-covariate INTERACTIONS\n# This lets the treatment effect vary by person\n\ncovariates = ['p2004_bin', 'p2002_bin', 'p2000_bin',\n              'g2004_bin', 'g2002_bin', 'g2000_bin',\n              'age', 'hh_size']\n\n# Fit separate logistic regressions per treatment arm\n# (equivalent to full interactions, but cleaner)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ntreatments = ['Control', 'Civic Duty', 'Hawthorne', 'Self', 'Neighbors']\nX_cols = covariates\n\n# Standardize for numerical stability\nscaler = StandardScaler()\nX_all = scaler.fit_transform(df[X_cols].values)\n\n# Fit one model per treatment arm: P(vote | X, A=a)\nmodels = {}\nfor t in treatments:\n    mask = df['treatment'] == t\n    X_t = X_all[mask]\n    y_t = df.loc[mask, 'voted_binary'].values\n    mod = LogisticRegression(max_iter=1000, solver='lbfgs')\n    mod.fit(X_t, y_t)\n    models[t] = mod\n    acc = mod.score(X_t, y_t)\n    print(f'{t:15s}: fitted on {mask.sum():>7,} obs, in-sample accuracy = {acc:.3f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 2: For each person, compute P(vote | a, x) for all treatments\n# Then h(x) = argmax_a P(vote | a, x)\n\npred_probs = pd.DataFrame(index=df.index)\nfor t in treatments:\n    pred_probs[t] = models[t].predict_proba(X_all)[:, 1]\n\n# Optimal assignment\ndf['optimal_treatment'] = pred_probs.idxmax(axis=1)\ndf['optimal_prob'] = pred_probs.max(axis=1)\n\n# What does the optimal policy look like?\nprint('Optimal treatment assignment h(x):')\nprint(df['optimal_treatment'].value_counts())\nprint()\nprint(f'Fraction assigned to Neighbors: {(df[\"optimal_treatment\"] == \"Neighbors\").mean():.1%}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 3: Hajek estimator for E_pi[Y]\n# pi(a|x) = 1[a = h(x)], so we use only obs where A_i = h(X_i)\n# Weight each by 1 / e(A_i) where e(a) = P(A=a) from randomization\n\n# Known propensities from the experimental design\npropensities = df['treatment'].map(\n    df['treatment'].value_counts(normalize=True)\n).values\n\n# Indicator: was this person assigned what the optimal policy would give them?\nmatched = (df['treatment'] == df['optimal_treatment']).values\ny = df['voted_binary'].values\n\n# Hajek estimator\nweights = matched / propensities\nE_pi = np.sum(weights * y) / np.sum(weights)\n\n# Compare to baselines\nE_control = df.loc[df['treatment'] == 'Control', 'voted_binary'].mean()\nE_neighbors = df.loc[df['treatment'] == 'Neighbors', 'voted_binary'].mean()\nE_overall = df['voted_binary'].mean()\n\nprint('=' * 60)\nprint('Policy Evaluation (Hajek IPW Estimator)')\nprint('=' * 60)\nprint(f'  E[vote | do nothing]  (Control):    {E_control:.4f}  ({E_control*100:.1f}%)')\nprint(f'  E[vote | random arm]  (as-ran):     {E_overall:.4f}  ({E_overall*100:.1f}%)')\nprint(f'  E[vote | Neighbors]   (best ATE):   {E_neighbors:.4f}  ({E_neighbors*100:.1f}%)')\nprint(f'  E[vote | h(x)]        (optimal):    {E_pi:.4f}  ({E_pi*100:.1f}%)')\nprint()\nprint(f'  Gain from optimal vs Control:        {(E_pi - E_control)*100:+.1f} pp')\nprint(f'  Gain from optimal vs Neighbors-all:  {(E_pi - E_neighbors)*100:+.1f} pp')\nprint()\n\nn_matched = matched.sum()\nn_eff = np.sum(weights)**2 / np.sum(weights**2)\nprint(f'  Matched observations: {n_matched:,} / {len(df):,} ({n_matched/len(df):.1%})')\nprint(f'  Effective sample size: {n_eff:,.0f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 4: Visualize -- who gets assigned what?\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: optimal assignment distribution\nax = axes[0]\nopt_counts = df['optimal_treatment'].value_counts().reindex(treatments)\ncolor_map = dict(zip(treatments, colors))\nbar_colors = [color_map[t] for t in opt_counts.index]\nbars = ax.bar(opt_counts.index, opt_counts.values, color=bar_colors, edgecolor='white')\nfor bar, count in zip(bars, opt_counts.values):\n    if count > 0:\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2000,\n                f'{count:,}\\n({count/len(df):.0%})', ha='center', fontsize=10)\nax.set_ylabel('Number of Individuals')\nax.set_title('Optimal Treatment Assignment h(x)')\nax.set_ylim(0, max(opt_counts.values) * 1.2)\n\n# Right: predicted P(vote) under optimal vs control\nax = axes[1]\nax.hist(pred_probs['Control'], bins=50, alpha=0.5, color='gray',\n        label=f'Under Control ({pred_probs[\"Control\"].mean()*100:.1f}% avg)', density=True)\nax.hist(df['optimal_prob'], bins=50, alpha=0.5, color='#e74c3c',\n        label=f'Under h(x) ({df[\"optimal_prob\"].mean()*100:.1f}% avg)', density=True)\nax.set_xlabel('Predicted P(vote)')\nax.set_ylabel('Density')\nax.set_title('Predicted Turnout: Control vs Optimal Policy')\nax.legend()\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 5: Learn an interpretable decision tree for h(x)\nfrom sklearn.tree import DecisionTreeClassifier, export_text\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nh_labels = le.fit_transform(df['optimal_treatment'])\n\n# Shallow tree for interpretability\ntree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=1000)\ntree.fit(df[X_cols], h_labels)\n\nprint('Decision Tree for Optimal Treatment Assignment')\nprint('=' * 60)\nprint(export_text(tree, feature_names=X_cols,\n                  class_names=le.classes_.tolist()))\nprint()\nprint(f'Tree accuracy on full data: {tree.score(df[X_cols], h_labels):.1%}')\nprint('(How well the simple tree approximates the full logistic argmax)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 6: Visualize the decision tree\nfrom sklearn.tree import plot_tree\n\nfig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(tree, feature_names=X_cols, class_names=le.classes_.tolist(),\n          filled=True, rounded=True, fontsize=9, ax=ax,\n          proportion=True, impurity=False)\nax.set_title('Interpretable Policy Tree: Which Mailer Should Each Voter Get?', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 7: Bootstrap confidence interval for the Hajek estimator\n# (because we should quantify uncertainty on E_pi[Y])\n\ndef hajek_estimate(y, treatment, optimal, propensities):\n    \"\"\"Hajek IPW estimator for deterministic policy.\"\"\"\n    matched = (treatment == optimal)\n    w = matched / propensities\n    return np.sum(w * y) / np.sum(w)\n\nnp.random.seed(42)\nn_boot = 2000\nboot_estimates = np.zeros(n_boot)\nn = len(df)\n\nfor b in range(n_boot):\n    idx = np.random.choice(n, size=n, replace=True)\n    boot_estimates[b] = hajek_estimate(\n        y[idx], df['treatment'].values[idx],\n        df['optimal_treatment'].values[idx], propensities[idx]\n    )\n\nci_lo, ci_hi = np.percentile(boot_estimates, [2.5, 97.5])\n\nprint('Bootstrap 95% CI for E_pi[Y]:')\nprint(f'  Point estimate:  {E_pi:.4f}  ({E_pi*100:.1f}%)')\nprint(f'  95% CI:          [{ci_lo:.4f}, {ci_hi:.4f}]  ([{ci_lo*100:.1f}%, {ci_hi*100:.1f}%])')\nprint()\nprint(f'  For comparison, Neighbors ATE:  {E_neighbors:.4f}  ({E_neighbors*100:.1f}%)')\nprint()\nif ci_lo > E_neighbors:\n    print('  The optimal policy SIGNIFICANTLY outperforms blanket Neighbors assignment.')\nelse:\n    print('  The optimal policy does not significantly outperform blanket Neighbors.')\n    print('  This suggests treatment effect heterogeneity is limited with these covariates.')\n    print('  (Neighbors is just the best treatment for almost everyone.)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 8: Targeting curve (Lorenz curve for treatment effects)\n#\n# For each person, compute the predicted GAIN from optimal treatment vs control:\n#   tau(x) = P(vote | h(x), x) - P(vote | control, x)\n#\n# Sort by tau descending. If you can only treat N people, treat\n# those with the highest tau. Plot cumulative extra votes vs N.\n\ntau = df['optimal_prob'] - pred_probs['Control']  # individual treatment effect\nsort_idx = tau.values.argsort()[::-1]  # descending\ntau_sorted = tau.values[sort_idx]\n\n# Cumulative extra votes if we treat the top N people\nN = np.arange(1, len(tau_sorted) + 1)\ncum_extra_votes = np.cumsum(tau_sorted)\n\n# Baselines for comparison:\n# 1. Random targeting: treat N random people with Neighbors\navg_neighbors_effect = (E_neighbors - E_control)\ncum_random = avg_neighbors_effect * N\n\n# 2. Perfect: same as our curve (it IS the model's best guess)\n# 3. No targeting: flat line at 0\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: cumulative extra votes\nax = axes[0]\nax.plot(N, cum_extra_votes, color='#e74c3c', linewidth=2, label='Optimal targeting h(x)')\nax.plot(N, cum_random, color='gray', linewidth=1.5, linestyle='--',\n        label=f'Random (Neighbors to all, +{avg_neighbors_effect:.3f}/person)')\nax.set_xlabel('Number of people treated (N)')\nax.set_ylabel('Cumulative extra votes (vs Control)')\nax.set_title('Targeting Curve: Cumulative Gain')\nax.legend(loc='lower right')\nax.set_xlim(0, len(df))\n\n# Annotate the gap\nmid = len(df) // 4\nax.annotate('Targeting\\ngain',\n            xy=(mid, cum_extra_votes[mid]),\n            xytext=(mid + 40000, cum_extra_votes[mid] - 3000),\n            arrowprops=dict(arrowstyle='->', color='#e74c3c'),\n            fontsize=10, color='#e74c3c')\n\n# Right: normalized (Lorenz-style)\n# x-axis: fraction of population treated\n# y-axis: fraction of TOTAL achievable extra votes captured\nax = axes[1]\nfrac_pop = N / len(df)\ntotal_extra = cum_extra_votes[-1]\nfrac_gain = cum_extra_votes / total_extra\n\nax.plot(frac_pop, frac_gain, color='#e74c3c', linewidth=2.5, label='Optimal targeting')\nax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random targeting (45-degree)')\nax.fill_between(frac_pop, frac_gain, frac_pop, alpha=0.15, color='#e74c3c')\n\n# Mark key points\nfor frac_target in [0.1, 0.25, 0.5]:\n    idx = int(frac_target * len(df)) - 1\n    gained = frac_gain[idx]\n    ax.plot(frac_target, gained, 'o', color='#e74c3c', markersize=8)\n    ax.annotate(f'{frac_target:.0%} of pop\\n= {gained:.0%} of gain',\n                xy=(frac_target, gained),\n                xytext=(frac_target + 0.05, gained - 0.08),\n                fontsize=9, arrowprops=dict(arrowstyle='->', color='gray'))\n\nax.set_xlabel('Fraction of population treated')\nax.set_ylabel('Fraction of total extra votes captured')\nax.set_title('Lorenz Curve for Targeting')\nax.legend(loc='lower right')\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n\n# Gini-style summary\ngini = 2 * (np.sum(frac_gain) / len(frac_gain) - 0.5)\nprint(f'Targeting Gini coefficient: {gini:.3f}')\nprint(f'(0 = no heterogeneity, everyone benefits equally; 1 = all benefit concentrated)')\nprint()\nprint('Key points:')\nfor frac_target in [0.1, 0.25, 0.5]:\n    idx = int(frac_target * len(df)) - 1\n    print(f'  Treating top {frac_target:.0%} captures {frac_gain[idx]:.0%} of total possible gain'\n          f'  ({cum_extra_votes[idx]:,.0f} extra votes)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Reading the Lorenz curve\n\nThe **left plot** shows raw cumulative extra votes as you expand the treated population. The gap between the red curve (optimal targeting) and the dashed line (random/uniform treatment) is the **value of targeting**: extra votes gained by being smart about who gets which mailer.\n\nThe **right plot** normalizes to fractions. The 45-degree line is random targeting (each additional person contributes the same expected gain). The red curve bows above it if there's heterogeneity: some people respond much more than others, so treating them first captures a disproportionate share of the total gain.\n\nThe **Gini coefficient** summarizes how much the curve bows. If it's near 0, everyone responds similarly and targeting doesn't help much. If it's large, there's real value in prioritizing who gets treated.\n\n**Prediction:** With only voting history, age, and household size as covariates, the Gini will be modest. These are blunt instruments. Richer covariates (neighborhood demographics, past campaign contact history, social network data) would likely reveal much more heterogeneity, and the Lorenz curve would bow further from the diagonal.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### What did we learn?\n\nThe optimal policy $h(x) = \\arg\\max_a \\hat{P}(\\text{vote} \\mid a, x)$ likely assigns Neighbors to almost everyone. This makes sense: the Neighbors treatment has the largest ATE (+8.1 pp), and the covariates available (past voting, age, household size) don't generate enough heterogeneity for other treatments to \"win\" for meaningful subgroups.\n\n**The real \"6 billion on the table\" question** is about *cost*. If the objective were:\n\n$$h(x) = \\arg\\max_a \\left[ \\hat{P}(\\text{vote} \\mid a, x) - \\lambda \\cdot \\text{cost}(a) \\right]$$\n\nwhere Neighbors mailers have higher social costs (angry neighbors, potential backlash), then the optimal policy *would* vary: send Neighbors only to people with the highest marginal benefit, and use cheaper treatments (Civic Duty, Hawthorne) for people who are nearly as responsive to lighter nudges.\n\nThis cost-sensitive framing is exactly what modern campaign targeting does, and what the uplift modeling literature (Athey, Imbens, etc.) formalized after 2008.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}